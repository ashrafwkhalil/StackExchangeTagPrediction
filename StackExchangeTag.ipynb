{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "# Spark imports\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import desc\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col, udf, array_contains\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType, IntegerType\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from csv import reader\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate the original dataset (obviously, it cannot be used). Take a look at https://stackoverflow.com/questions/13793529/r-error-invalid-type-list-for-variable to see how useless the Body column information could be!\n",
    "\n",
    "The point here is that the body information consists mostly of codes and some weird patterns that are not useful for our purpose. The most important information here is the connection between the title of the questions and tags. So, I removed the Body column from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark = init_spark()\n",
    "\n",
    "    filename1 = \"./Train.csv\"\n",
    "    df2 = spark.read.option(\"multiLine\", 'true').option(\"escape\",\"\\'\").csv(filename1, header=True)\n",
    "    print(df2.count())\n",
    "    print(df2.show(10))    \n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For removing the Body column, I read all the dataset once using Pandas library. After that, I removed the column and got an export to have a concrete file as our dataset. This part has been ommited from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = init_spark()\n",
    "\n",
    "filename = \"./TrainWithoutBody.csv\"\n",
    "df1 = spark.read.option(\"multiLine\", 'true').option(\"escape\",\"\\'\").csv(filename, header=True)\n",
    "df1 = df1.drop(\"_c0\")\n",
    "df1 = df1.dropna()\n",
    "\n",
    "rddTags = df1.select(\"Tags\").rdd\n",
    "\n",
    "# df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the 100 most used tags (one DT per each most used tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splittedTags = rddTags.filter(lambda r: r[0] != None).flatMap(lambda r: r[0].split(\" \")).map(lambda r: r.replace(\".\", \"\")).map(lambda r: (r, 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "splittedTags = splittedTags.sortBy(lambda r: r[1], False) #Sorted with number of usage (you can collect and see)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag v/s Count Distribution for all tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagCountDF = splittedTags.toDF([\"tag\", \"count\"])\n",
    "tagCountDF.show()\n",
    "cCount = tagCountDF.select(\"count\").collect()\n",
    "plt.plot([i for i in range(splittedTags.count())], cCount)\n",
    "plt.title(\"Tag v/s Count distribution\")\n",
    "plt.ylabel(\"Tag Count\")\n",
    "plt.xlabel(\"Tag ID\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag v/s Count Distribution for the top 500 tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(500)], cCount[:500])\n",
    "plt.title(\"Tag v/s Count distribution for first 500 tags\")\n",
    "plt.ylabel(\"Tag Count\")\n",
    "plt.xlabel(\"Tag ID\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag v/s Count Distribution for the top 100 tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(100)], cCount[:100])\n",
    "plt.title(\"Tag v/s Count distribution for first 100 tags\")\n",
    "plt.ylabel(\"Tag Count\")\n",
    "plt.xlabel(\"Tag ID\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag v/s Count Distribution for the 10 best tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cTag = tagCountDF.select(\"tag\").take(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6), dpi=80)\n",
    "plt.bar([i for i in range(10)], [r.__getitem__(\"count\") for r in cCount[:10]], tick_label=[r.__getitem__(\"tag\") for r in cTag])\n",
    "plt.title(\"Tag v/s Count distribution for first 10 tags\")\n",
    "plt.ylabel(\"Tag Count\")\n",
    "plt.xlabel(\"Tag ID\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splittedTagsSorted = splittedTags.map(lambda r: r[0]) #Delete this line if you want to see number of times they have been used.\n",
    "\n",
    "mostUsedTags = splittedTagsSorted.collect()[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostUsedTags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.sample(0.001, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up tags to include only most-used tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanUpTags(tags):\n",
    "    tags = tags.split(\" \")\n",
    "    tags = [tag for tag in tags if tag in mostUsedTags]\n",
    "    return tags\n",
    "\n",
    "cleanUpTagsUDF = udf(cleanUpTags, ArrayType(StringType()))\n",
    "\n",
    "df1 = df1.withColumn(\"cleantags\",  cleanUpTagsUDF(col(\"tags\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove questions that do not include the top 50 tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterEmptyRows(tags):\n",
    "    return len(tags)\n",
    "\n",
    "filterEmptyRowsUDF = udf(filterEmptyRows, IntegerType())\n",
    "\n",
    "df1 = df1.filter(filterEmptyRowsUDF(col(\"cleantags\")) > 0)\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject titles to TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"Title\", outputCol=\"transformed_tfidf\")\n",
    "wordsData = tokenizer.transform(df1)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"transformed_tfidf\", outputCol=\"rawFeatures\")\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "df1 = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add 0/1 column for each tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in mostUsedTags:\n",
    "    df1 = df1.withColumn(tag, (array_contains(col(\"cleantags\"), tag)).cast('integer'))\n",
    "\n",
    "df.select(\"cleantags\", \"Javascript\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data to train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df1.randomSplit([.7,.3],seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DecisionTreeClassifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Handling class imbalance using undersampling method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def underSample(training_data, etag):\n",
    "    major_df = training_data.filter(col(etag) == 0)\n",
    "    minor_df = training_data.filter(col(etag) == 1)\n",
    "    ratio = int(major_df.count()/minor_df.count())\n",
    "    \n",
    "    sampled_majority_df = major_df.sample(False, 1/ratio)\n",
    "    train_data = sampled_majority_df.unionAll(minor_df)\n",
    "    \n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Handling class imbalance with assigning class weights (higher weight to minority class) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classWeight(training_data, etag):\n",
    "    balancingRatio = training_data.filter(col(etag) == 1).count() / training_data.count()\n",
    "    calculateWeights = udf(lambda x: 1 * balancingRatio if x == 0 else (1 * (1.0 - balancingRatio)), DoubleType())\n",
    "    \n",
    "    training_data = training_data.withColumn(\"classWeightCol\", calculateWeights(col(etag)))\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWithUndersampling(training_data, etag):\n",
    "    \n",
    "    # training_data = underSample(training_data, etag) #Commented for now\n",
    "    training_data = classWeight(training_data, etag) #Can be commented for speedup\n",
    "\n",
    "    model = DecisionTreeClassifier(featuresCol=\"features\", labelCol=etag, maxDepth=4, impurity=\"gini\", weightCol=\"classWeightCol\").fit(training_data)\n",
    "\n",
    "    # Using model without assigning classweight\n",
    "    # model = DecisionTreeClassifier(featuresCol=\"features\", labelCol=etag, maxDepth=4, impurity=\"gini\").fit(training_data)\n",
    "    pred = model.transform(test_data)\n",
    "    \n",
    "    tp = pred.filter((col(etag) == 1) & (col(\"prediction\") == 1)).count()\n",
    "    fp = pred.filter((col(etag) == 0) & (col(\"prediction\") == 1)).count()\n",
    "    fn = pred.filter((col(etag) == 1) & (col(\"prediction\") == 0)).count()\n",
    "    \n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision / (precision + recall))\n",
    "    \n",
    "    return model, pred, precision * 100, recall * 100, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models for each tag and obtain precision, recall, and f-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtcs = defaultdict()\n",
    "preds = defaultdict()\n",
    "precisions = defaultdict()\n",
    "recalls = defaultdict()\n",
    "fmeasures = defaultdict()\n",
    "\n",
    "for tag in mostUsedTags:\n",
    "    if tag not in dtcs:\n",
    "        dtcs[tag], preds[tag], precisions[tag], recalls[tag], fmeasures[tag] = trainWithUndersampling(train_data, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(precisions)\n",
    "# print(recalls)\n",
    "# print(fmeasures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject titles to Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use df as your dataframe and add your column to it "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
